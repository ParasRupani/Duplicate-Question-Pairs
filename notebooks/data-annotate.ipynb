{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Annotation - Sentence paraphrasing**\n",
    "\n",
    "After gathering and combining the scraped data into a single file, we used a paraphraser like \"chatgpt_paraphraser_on_T5_base\" from Hugging Face to create different versions of the questions we've collected. This helps in forming pairs of annotated questions. The paraphraser rephrases the original questions with varied wording and structure. For each original question, we generate five duplicate questions. We then pair the first two duplicates with the original question to form similar question pairs. The remaining three duplicates are shuffled to create a set of dissimilar questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library as pd for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file containing the merged questions data into a DataFrame named df\n",
    "df = pd.read_csv(\"data\\merged_questions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paraphraser Reference: https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base\n",
    "\n",
    "# Import necessary modules from the transformers library\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Set the device to use for processing\n",
    "device = \"cuda\"\n",
    "\n",
    "# Load the tokenizer from the pre-trained \"chatgpt_paraphraser_on_T5_base\" model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "\n",
    "# Load the model for sequence-to-sequence generation from the pre-trained \"chatgpt_paraphraser_on_T5_base\" model\n",
    "# and move it to the specified device (GPU or CPU)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
    "\n",
    "# Define a function for paraphrasing input questions\n",
    "def paraphrase(\n",
    "    question,\n",
    "    num_beams=5,\n",
    "    num_beam_groups=5,\n",
    "    num_return_sequences=5,\n",
    "    repetition_penalty=10.0,\n",
    "    diversity_penalty=3.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=128\n",
    "):\n",
    "    # Tokenize the input question and prepare it for model input\n",
    "    input_ids = tokenizer(\n",
    "        f'paraphrase: {question}',  # Add prefix \"paraphrase:\" to indicate paraphrasing\n",
    "        return_tensors=\"pt\", padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids.to(device)\n",
    "    \n",
    "    # Generate paraphrases using the loaded model\n",
    "    outputs = model.generate(\n",
    "        input_ids, temperature=temperature, repetition_penalty=repetition_penalty,\n",
    "        num_return_sequences=num_return_sequences, no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams, num_beam_groups=num_beam_groups,\n",
    "        max_length=max_length, diversity_penalty=diversity_penalty\n",
    "    )\n",
    "\n",
    "    # Decode the generated outputs and remove special tokens\n",
    "    res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # Return the paraphrased results\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1000/17100 [11:02<2:52:06,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2000/17100 [23:45<4:39:20,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3000/17100 [34:17<2:41:55,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 4000/17100 [44:56<2:06:03,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 5000/17100 [54:24<2:10:37,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 6000/17100 [1:03:37<1:36:15,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 7000/17100 [1:13:00<2:11:08,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 8000/17100 [1:23:57<1:21:10,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 9000/17100 [1:34:07<1:51:22,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 10000/17100 [1:43:56<58:08,  2.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 11000/17100 [1:53:46<57:49,  1.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 12000/17100 [2:02:57<43:02,  1.98it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 13000/17100 [2:12:41<30:36,  2.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 14000/17100 [2:23:33<33:41,  1.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 15000/17100 [2:34:48<24:31,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 16000/17100 [2:48:01<12:44,  1.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 17000/17100 [3:01:00<01:10,  1.41it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17100/17100 [3:02:07<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Import tqdm library for progress tracking\n",
    "import tqdm\n",
    "\n",
    "# Ignore warnings to avoid cluttering the output\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize an empty list to store paraphrased question pairs\n",
    "l = []\n",
    "\n",
    "# Intialy we did a sanity check, which run has already created paraphrases for first 3000 samples and were saved.\n",
    "# Now we generate para[hrases for rest of the questions by looping over indices starting from 3000 until the length of the DataFrame\n",
    "for ind in tqdm.tqdm(range(3000, len(df))):\n",
    "    \n",
    "    # Print progress message every 1000 iterations\n",
    "    if (ind % 1000) == 0:\n",
    "        print(ind)\n",
    "        l = []\n",
    "    \n",
    "    try:\n",
    "        # Get the text of the question from the DataFrame\n",
    "        text = df.loc[ind, \"q1\"]\n",
    "        \n",
    "        # Generate paraphrases for the question text using the paraphrase function\n",
    "        l.append([ind, text] + paraphrase(text))\n",
    "    \n",
    "    except:\n",
    "        # Print an error message if an exception occurs during paraphrasing\n",
    "        print(\"ERROR!!! @ \", ind)\n",
    "    \n",
    "    # Save the paraphrased question pairs to a CSV file every 1000 iterations\n",
    "    if ((ind + 1) % 1000) == 0:\n",
    "        d = pd.DataFrame(l, columns=[\"idx\", \"q1\", \"d1\", \"d2\", \"d3\", \"d4\", \"d5\"])\n",
    "        d.to_csv(\"pairs/pairs_{}.csv\".format(ind))\n",
    "    \n",
    "    # Save the paraphrased question pairs to a CSV file when reaching the end of the DataFrame\n",
    "    if ind == len(df):\n",
    "        d = pd.DataFrame(l, columns=[\"idx\", \"q1\", \"d1\", \"d2\", \"d3\", \"d4\", \"d5\"])\n",
    "        d.to_csv(\"pairs/pairs_{}.csv\".format(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
